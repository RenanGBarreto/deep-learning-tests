{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 2 - 1ª Lista de Exercícios - Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[-0.7882065021219178, -0.4237362213716127],\n",
       "  [0.2799959164121657, 0.8367772328918941],\n",
       "  [0.04095476820171862, -0.3855082152750544],\n",
       "  [-0.14324598756125884, -0.7468332993108617],\n",
       "  [-0.20338592666356137, 0.8059895954756291],\n",
       "  [0.48433697267693576, 0.07396888221068032],\n",
       "  [-0.1352653359880862, 0.2592901355113544],\n",
       "  [0.5405980198814064, 0.6096855864132174]],\n",
       " [[0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def randomizeArrayData_Q4(arraySize):\n",
    "    \"\"\" \n",
    "    Creates a new array of float pairs (vectors) between -1 and 1, corresponding to X and Y coordinates\n",
    "    of the points inside the circle\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    \n",
    "    for i in range(arraySize):\n",
    "        arr.append([(np.random.uniform(-1.0, 1.0)), (np.random.uniform(-1.0, 1.0))])\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def checkPointLine(x1, y1, x2, y2, xp, yp):\n",
    "    \"\"\" \n",
    "    Checks if a point (xp, yp) is above or below a line defined by other tho points (x1,y1) and (x2,y2)\n",
    "    \"\"\"\n",
    "    v1 = [x2-x1, y2-y1]   # Vector 1\n",
    "    v2 = [xp-x1, yp-y1]   # Vector 2\n",
    "    crossProduct = v1[0]*v2[1] - v1[1]*v2[0]\n",
    "    \n",
    "    #if crossProduct > 0:\n",
    "        #print 'point is on the counter-clockwise side of line'\n",
    "    #elif crossProduct < 0:\n",
    "        #print 'point is on the clockwise side of line'\n",
    "    #else:\n",
    "        #print 'point is exactly on the line'\n",
    "        \n",
    "    return crossProduct\n",
    "\n",
    "\n",
    "def generateDataset_Q4(datasetSize):\n",
    "    \"\"\" \n",
    "    Generates dataset for XOR function training\n",
    "    \"\"\"\n",
    "    \n",
    "    X = randomizeArrayData_Q4(datasetSize)\n",
    "    \n",
    "    # X below is for debugging reasons\n",
    "    #X = [[0.3, 0.3], [0.9, 0.9], [0.3, -0.3], [0.9, -0.9], [-0.3, 0.3], [-0.9, 0.9], [-0.3, -0.3], [-0.9, -0.9]]\n",
    "    \n",
    "    Y = []\n",
    "    \n",
    "    # Creates datasetSize examples\n",
    "    for i in range(datasetSize):\n",
    "        if X[i][0] >= 0:\n",
    "            \n",
    "            if X[i][1] >= 0:\n",
    "                \n",
    "                if checkPointLine(0.0, 1.0, 1.0, 0.0, X[i][0], X[i][1]) >= 0: # Points for the line on the upper right quadrant\n",
    "                    Y.append([0,0,0,0,1,0,0,0]) #Point is above line, so its on C5 region                    \n",
    "                else:\n",
    "                    Y.append([1,0,0,0,0,0,0,0]) #Point is below line, so its on C1 region\n",
    "                    \n",
    "                #print('Superior right')\n",
    "            else:\n",
    "                \n",
    "                if checkPointLine(0.0, -1.0, 1.0, 0.0, X[i][0], X[i][1]) <= 0: # Points for the line on the lower right quadrant\n",
    "                    Y.append([0,0,0,0,0,0,0,1]) #Point is below line, so its on C8 region\n",
    "                    \n",
    "                else:\n",
    "                    Y.append([0,0,0,1,0,0,0,0]) #Point is above line, so its on C4 region\n",
    "                    \n",
    "                #print('Inferior right')\n",
    "            \n",
    "        else:\n",
    "            if X[i][1] >= 0:\n",
    "                \n",
    "                if checkPointLine(0.0, 1.0, -1.0, 0.0, X[i][0], X[i][1]) <= 0: # Points for the line on the upper left quadrant\n",
    "                    Y.append([0,0,0,0,0,1,0,0]) #Point is above line, so its on C6 region\n",
    "                    \n",
    "                else:\n",
    "                    Y.append([0,1,0,0,0,0,0,0]) #Point is below line, so its on C2 region\n",
    "                    \n",
    "                #print('Superior left')\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                if checkPointLine(0.0, -1.0, -1.0, 0.0, X[i][0], X[i][1]) >= 0: # Points for the line on the lower left quadrant\n",
    "                    Y.append([0,0,0,0,0,0,1,0]) #Point is below line, so its on C7 region\n",
    "                    \n",
    "                else:\n",
    "                    Y.append([0,0,1,0,0,0,0,0]) #Point is above line, so its on C3 region\n",
    "                \n",
    "                #print('Inferior left')\n",
    "                \n",
    "    return X, Y\n",
    "\n",
    "#display(randomizeArrayData_Q4(10))\n",
    "display(generateDataset_Q4(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def randomizeArrayData_Q3B(arraySize):\n",
    "    \"\"\" \n",
    "    Creates a new array of random float numbers between 0 and 4\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    \n",
    "    for i in range(arraySize):\n",
    "        arr.append(np.random.uniform(0.0, 4.0))\n",
    "        \n",
    "    return arr\n",
    "\n",
    "\n",
    "def generateDataset_Q3B(datasetSize):\n",
    "    \"\"\" \n",
    "    Generates dataset for training following Q3B function format\n",
    "    \"\"\"\n",
    "    \n",
    "    X = randomizeArrayData_Q3B(datasetSize)\n",
    "    Y = []\n",
    "    \n",
    "    # Creates datasetSize examples\n",
    "    for i in range(datasetSize):\n",
    "        Y.append((np.sin(np.pi * X[i])) / (np.pi * X[i]))\n",
    "        \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Questão 1](imgs/questoes/q2.png \"Questão 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criação de um dataset com 16 pontos e variação de +/- 20%:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[-0.09691459000933111, -0.0486568520620428, -0.0962018093521039],\n",
       "  [-0.03886165138864637, -0.07001981952578444, 1.0224819581093567],\n",
       "  [-0.07043296153698611, 0.9152454390333591, 0.06850423690646681],\n",
       "  [0.015258354004163, 0.9440231203531525, 1.0034215319342255],\n",
       "  [1.0199599414360074, -0.03843508917259271, -0.014726914426950843],\n",
       "  [0.931054606760386, 0.06492858929525668, 0.9398273861435202],\n",
       "  [1.0521363478444345, 0.9960209256771867, -0.07077731354637],\n",
       "  [1.0031555206800964, 0.9681286511626965, 0.9448166098601924],\n",
       "  [0.06867663772305237, 0.06141577851834732, -0.08676331061185799],\n",
       "  [0.05732931316282852, 0.004221245349283209, 0.9750079250622291],\n",
       "  [-0.07616718515155688, 0.9332806642891078, 0.02650088092975065],\n",
       "  [0.018693603540878123, 1.0346732106667087, 0.9898600935961733],\n",
       "  [1.0130257360590214, -0.0877114828315003, -0.08958072235274402],\n",
       "  [1.0232240193955744, 0.06933168662904657, 1.0019450216226913],\n",
       "  [1.0440137340271685, 0.9679680555261122, 0.0728199334839923],\n",
       "  [0.9117592711674871, 1.065370143787349, 0.9571715599912617]],\n",
       " [[0],\n",
       "  [1],\n",
       "  [2],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5],\n",
       "  [6],\n",
       "  [7],\n",
       "  [0],\n",
       "  [1],\n",
       "  [2],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5],\n",
       "  [6],\n",
       "  [7]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def randomizeArrayData(arr, variation=0.1):\n",
    "    \"\"\" \n",
    "    Adiciona uma variação aleatoria em uma array de floats \n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = arr[i] + (np.random.random()-0.5)*variation\n",
    "        \n",
    "    return arr\n",
    "\n",
    "def criaDataset(n, variation=0.1):\n",
    "    \"\"\" \n",
    "    Cria um conjunto de exemplos para a questão 1. Classificação de arestas em um cubo \n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    # Crie n/8 exemplos para cada classe\n",
    "    for i in range(math.ceil(n/8)):\n",
    "        X.append(randomizeArrayData([0, 0, 0], variation)) \n",
    "        Y.append([0])\n",
    "        X.append(randomizeArrayData([0, 0, 1], variation)) \n",
    "        Y.append([1])\n",
    "        X.append(randomizeArrayData([0, 1, 0], variation)) \n",
    "        Y.append([2])\n",
    "        X.append(randomizeArrayData([0, 1, 1], variation)) \n",
    "        Y.append([3])\n",
    "        X.append(randomizeArrayData([1, 0, 0], variation)) \n",
    "        Y.append([4])\n",
    "        X.append(randomizeArrayData([1, 0, 1], variation)) \n",
    "        Y.append([5])\n",
    "        X.append(randomizeArrayData([1, 1, 0], variation)) \n",
    "        Y.append([6])\n",
    "        X.append(randomizeArrayData([1, 1, 1], variation)) \n",
    "        Y.append([7])\n",
    "        \n",
    "    return X, Y\n",
    "\n",
    "print(\"Criação de um dataset com 16 pontos e variação de +/- 20%:\\n\")\n",
    "display(criaDataset(16, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 1],\n",
       "  [0, 0],\n",
       "  [0, 1],\n",
       "  [0, 1],\n",
       "  [0, 1],\n",
       "  [1, 1],\n",
       "  [0, 1],\n",
       "  [1, 0],\n",
       "  [1, 1],\n",
       "  [1, 0]],\n",
       " [[1], [0], [1], [1], [1], [0], [1], [1], [0], [1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def randomizeArrayData_Q3A(arraySize):\n",
    "    \"\"\" \n",
    "    Creates a new array of boolean pairs in int format\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    \n",
    "    for i in range(arraySize):\n",
    "        arr.append([(np.random.randint(0,2)),(np.random.randint(0,2))])\n",
    "        \n",
    "    return arr\n",
    "\n",
    "\n",
    "def generateDataset_Q3A(datasetSize):\n",
    "    \"\"\" \n",
    "    Generates dataset for XOR function training\n",
    "    \"\"\"\n",
    "    \n",
    "    X = randomizeArrayData_Q3A(datasetSize)\n",
    "    Y = []\n",
    "    \n",
    "    # Creates datasetSize examples\n",
    "    for i in range(datasetSize):\n",
    "        if X[i] == [0, 0]: \n",
    "            Y.append([0])\n",
    "        if X[i] == [0, 1]: \n",
    "            Y.append([1])\n",
    "        if X[i] == [1, 0]: \n",
    "            Y.append([1])\n",
    "        if X[i] == [1, 1]: \n",
    "            Y.append([0])\n",
    "        \n",
    "    return X, Y\n",
    "\n",
    "display(generateDataset_Q3A(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "from viznet import connecta2a, connect121, node_sequence, NodeBrush, EdgeBrush, DynamicShow\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pdb\n",
    "pdb.set_trace = lambda: 1  # This solves a problem with the python debugger and the library viznet\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"\n",
    "    Class that represents a neuron inside the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs, activation='sigmoid'):   \n",
    "        \"\"\"\n",
    "        Create a neuron object.\n",
    "        Param:\n",
    "            - inputs: THe number of inputs that this neuron will have.\n",
    "            - activation: The name of the activation function\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.weights = np.random.rand(inputs)\n",
    "        self.bias = 1\n",
    "        \n",
    "        self.newWeights = self.weights\n",
    "        \n",
    "        assert inputs > 0\n",
    "        self.inputs = int(inputs)\n",
    "        \n",
    "        assert activation == 'sigmoid' or activation == 'tanh' or activation == 'relu' or activation == 'linear'\n",
    "        self.activation = activation\n",
    "\n",
    "    def activation_function(self, func_type, value):\n",
    "        \"\"\"\n",
    "        Most commons activation functions.\n",
    "        Param: func_type - A tring with one of the valid values: sigmoid, tanh, relu, linear\n",
    "                    value - Apply the function to the value\n",
    "        Return: The result of the activation function\n",
    "        \"\"\"\n",
    "        \n",
    "        assert func_type == 'sigmoid' or func_type == 'tanh' or func_type == 'relu' or func_type == 'linear'\n",
    "        \n",
    "        if func_type == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-value))\n",
    "        elif func_type == 'tanh':\n",
    "            return math.sinh(value) / math.cosh(value)\n",
    "        elif func_type == 'relu':\n",
    "            return 0 if (value < 0) else value\n",
    "        elif func_type == 'linear':\n",
    "            return value    \n",
    "    \n",
    "    def adjustWeights(self):\n",
    "        \"\"\"\n",
    "        Update the weigths with the pre-calculated newWeigths variable. This function is used during training.\n",
    "        \"\"\"\n",
    "        self.weights = self.newWeights\n",
    "        self.newWeights = self.weights\n",
    "        \n",
    "    \n",
    "    def foward(self, x, apply_activation=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Calculate the output of the neuron. \n",
    "        Param:\n",
    "            - inputs: Array with the inputs signals\n",
    "            - apply_activation: If true, Apply the activation final. If false, return the output without activation function (Vj)\n",
    "            - verbose: If true, show some data on the stdout\n",
    "        Return: The neuron output (with or without the activation function applied)\n",
    "        \"\"\"\n",
    "        vj = np.dot(x, self.weights) + self.bias\n",
    "        out = vj if (not apply_activation) else self.activation_function(self.activation , vj)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Input: \", x, \" -> Vj: \", vj, \" -> Output: \", out)\n",
    "            \n",
    "        return out\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Alternative way to see the object as string\n",
    "        \"\"\"\n",
    "        return pprint.pformat(vars(self), indent=0, width=1000, depth=None)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Class that represents a Feed-foward Neural Network that can be trained using backpropagation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inputs=1, architecture=[1], lr=0.01, momentum=0, isClassification=False, activation='sigmoid', activation_last_layer='linear'):\n",
    "        \"\"\"\n",
    "        Create a feed-foward neural network object.\n",
    "        Param:\n",
    "            - inputs: The amount of inputs the network will have (scalar)\n",
    "            - architecture: An array of numbers that represents the amount of neurons in each layer            \n",
    "            - lr: The learning rate\n",
    "            - momentum: The momentum constant. Zero if we shall not momentum\n",
    "            - isClassification: If true, use a int+round function that is applyed in the end of the calculation\n",
    "            - activation: The name of the activation function for all hidden and input layers\n",
    "            - activation_last_layer: Activation function for the last layer of the model\n",
    "        \"\"\"\n",
    "        \n",
    "        # Make sure the architecture is a list and not empty\n",
    "        assert isinstance(architecture, list) and len(architecture) > 0\n",
    "        \n",
    "        self.architecture = architecture        \n",
    "        assert inputs > 0\n",
    "        self.inputs = int(inputs)\n",
    "        self.momentum = momentum if momentum > 0 else 0\n",
    "        self.lr = lr if lr > 0 else 0.00001 # Make sure the LR is not too close to zero       \n",
    "        self.isClassification = isClassification        \n",
    "        self.activation = activation\n",
    "        self.activation_last_layer = activation_last_layer\n",
    "\n",
    "        # Create the layers and neurons based on the architecture\n",
    "        self.initLayers()\n",
    "        \n",
    "    def initLayers(self):\n",
    "        \"\"\"\n",
    "        Initialize the layers and create the neurons\n",
    "        \"\"\"\n",
    "        \n",
    "        # The amount of inputs of each Neuron need to be the total of neurons of the previus layer OR self.inputs if this is the first layer\n",
    "        totalInputs = self.inputs\n",
    "        \n",
    "        # Clear all layers (and weigths)\n",
    "        self.layers = []\n",
    "        \n",
    "        # For each layer of the architecture\n",
    "        for lIdx in range(len(self.architecture)):  \n",
    "            \n",
    "            # Start an array with the current layer\n",
    "            currentLayer = []\n",
    "            \n",
    "            # The last layer can have a different activation function\n",
    "            isLastLayer = (lIdx == len(self.architecture)-1)\n",
    "            \n",
    "            # Create n neurons \n",
    "            for count in range(self.architecture[lIdx]):\n",
    "                \n",
    "                currentLayer.append( Neuron(totalInputs, activation=(self.activation_last_layer if isLastLayer else self.activation)))\n",
    "                \n",
    "            totalInputs = self.architecture[lIdx]\n",
    "            \n",
    "            # Add the current layer to the layer list\n",
    "            self.layers.append(currentLayer)        \n",
    "     \n",
    "    def activation_function_derivative(self, func_type, value):\n",
    "        \"\"\"\n",
    "        Most commons activation functions.\n",
    "        Param: func_type - A string with one of the valid values: sigmoid, tanh, relu, linear and step\n",
    "                    value - Apply the derivative function to the value\n",
    "        Return: The result of the derivative activation function\n",
    "        \"\"\"\n",
    "        \n",
    "        assert func_type == 'sigmoid' or func_type == 'tanh' or func_type == 'relu' or func_type == 'linear'\n",
    "        \n",
    "        if func_type == 'sigmoid':\n",
    "            f = 1/(1+np.exp(-value))\n",
    "            df = f * (1 - f)\n",
    "            return df\n",
    "        elif func_type == 'tanh':\n",
    "            f = math.sinh(value) / math.cosh(value)\n",
    "            df = (1-(f*f))/2\n",
    "            return df\n",
    "        elif func_type == 'relu':\n",
    "            return 0 if (value < 0) else 1\n",
    "        elif func_type == 'linear':\n",
    "            return 1\n",
    "\n",
    "        \n",
    "    def evaluate(self, x=None, y=None):\n",
    "        \"\"\"\n",
    "        Returns the loss value for each data.\n",
    "        Param:\n",
    "            x: The input samples data, as a Numpy array (or list of Numpy arrays).\n",
    "            y: The target values\n",
    "        Return: An array of loss values \n",
    "        \"\"\"\n",
    "        # TODO \n",
    "        return None        \n",
    "    \n",
    "    def predict(self, x, verbose=False):\n",
    "        \"\"\"\n",
    "        Create the predictions for the input samples.\n",
    "        Param:\n",
    "            x: One input data\n",
    "            verbose: If true, show some data on the stdout\n",
    "        Return: the predicted value\n",
    "        \"\"\"\n",
    "\n",
    "        # On the first layer, the input is the data row\n",
    "        currentInputs = x\n",
    "\n",
    "        currentOutputs = []\n",
    "\n",
    "        # For each layer\n",
    "        for l in range(len(self.layers)):\n",
    "            currentOutputs = []\n",
    "            \n",
    "            currentLayer = self.layers[l]\n",
    "\n",
    "            # For each neuron \n",
    "            for n in range(len(self.layers[l])):\n",
    "                currentNeuron = self.layers[l][n]                    \n",
    "                currentOutputs.append(currentNeuron.foward(currentInputs, apply_activation=True, verbose=verbose))\n",
    "\n",
    "            # For the next layer, the input become the output of the preview layer\n",
    "            currentInputs = currentOutputs\n",
    "        \n",
    "        # Apply the output filter if needed\n",
    "        #for i in range(len(currentOutputs)):\n",
    "        #    currentOutputs[i] = currentOutputs[i] if not self.isClassification else int(round(currentOutputs[i]))\n",
    "        \n",
    "        \n",
    "        if self.isClassification:\n",
    "            biggestIdx = 0\n",
    "            biggestValue = -1\n",
    "            for idx in range(len(currentOutputs)):\n",
    "                if currentOutputs[idx] > biggestValue:\n",
    "                    biggestValue = currentOutputs[idx]\n",
    "                    biggestIdx = idx\n",
    "            \n",
    "            for idx in range(len(currentOutputs)):\n",
    "                currentOutputs[idx] = 1 if idx == biggestIdx else 0\n",
    "                \n",
    "        return currentOutputs\n",
    "    \n",
    "    def save(self, file='neuralnet.pkl'):\n",
    "        \"\"\"\n",
    "        Serialize the Neural Network to a file that can be loaded later.\n",
    "        Param:\n",
    "            file: The file path where the object will be saved\n",
    "        \"\"\"\n",
    "        assert file != None\n",
    "        with open(file, 'wb') as output:  # Overwrites any existing file.\n",
    "            pickle.dump(self, output, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def load(self, file='neuralnet.pkl'):\n",
    "        \"\"\"\n",
    "        Serialize the Neural Network to a file that can be loaded later.\n",
    "        Param:\n",
    "            file: The file path where the object will be saved\n",
    "        Return: This object with the new architecture\n",
    "        \"\"\"\n",
    "        assert file != None\n",
    "        with open(file, 'rb') as input:\n",
    "            tmp = pickle.load(input)\n",
    "        self.__dict__.update(tmp.__dict__)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def draw(self, file=None, size=(10,6)):\n",
    "        \"\"\"\n",
    "        Draw the network architecture \n",
    "        Param:\n",
    "            - file: The file where the image will be saved. Default: None\n",
    "            - size: the image size. Default: (10,6)\n",
    "        \"\"\"\n",
    "        \n",
    "        with DynamicShow(size, filename=file) as d:\n",
    "            \n",
    "            num_hidden_layer = len(self.architecture) - 2\n",
    "            token_list = ['\\sigma^z'] + ['y^{(%s)}' % (i + 1) for i in range(num_hidden_layer)] + ['\\psi']\n",
    "            kind_list = ['nn.input'] + ['nn.hidden'] * num_hidden_layer + ['nn.output']\n",
    "            radius_list = [0.1] + [0.2] * num_hidden_layer + [0.3]\n",
    "            x_list = 1.5 * np.arange(len(self.architecture)) + 1.5\n",
    "\n",
    "            seq_list = []\n",
    "            \n",
    "            # Input pins\n",
    "            inputPins = NodeBrush('qc.C', d.ax)\n",
    "            seq_list.append(node_sequence(inputPins, self.inputs, center=(0, 0), space=(0, 1)))\n",
    "            \n",
    "            # Network and connections\n",
    "            for n, kind, radius, y in zip(self.architecture, kind_list, radius_list, x_list):\n",
    "                b = NodeBrush(kind, d.ax)\n",
    "                seq_list.append(node_sequence(b, n, center=(y, 0), space=(0, 1)))\n",
    "            \n",
    "            for st, et in zip(seq_list[:-1], seq_list[1:]):\n",
    "                connecta2a(st, et, EdgeBrush('-->', d.ax))\n",
    "            \n",
    "            # Output pins\n",
    "            outputEdge = EdgeBrush('---', d.ax)\n",
    "            outputPins = NodeBrush('qc.C', d.ax)\n",
    "            seq_list.append(node_sequence(outputPins, self.architecture[-1], center=(x_list[-1]+1.5, 0), space=(0, 1)))            \n",
    "            connect121( seq_list[-2], seq_list[-1], outputEdge)                \n",
    "                    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Alternative way to see the object as string\n",
    "        \"\"\"\n",
    "        return pprint.pformat(vars(self), indent=1, width=1, depth=5)\n",
    "    \n",
    "    def fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=True, validation_split=0.1, shuffle=True):\n",
    "        \"\"\"\n",
    "        Train the Neural Network\n",
    "        Param:\n",
    "            x: The input samples data, as a Numpy array (or list of Numpy arrays).\n",
    "        Return: Numpy array(s) of predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        X_train = x        \n",
    "        y_train = y\n",
    "        \n",
    "        ## TODO shuffle and split the validation dataset\n",
    "        X_val = x \n",
    "        y_val = y\n",
    " \n",
    "        # Discover the corect bash size\n",
    "        if batch_size == None or batch_size > len(X_train):\n",
    "            batch_size = len(X_train)\n",
    "            \n",
    "        # For each epoch\n",
    "        for e in range(epochs):\n",
    "            \n",
    "            # TODO shuffle\n",
    "            \n",
    "            # For each example\n",
    "            for idx in range(len(X_train)):\n",
    " \n",
    "            \n",
    "                # Foward pass ##########\n",
    "\n",
    "                # For each layer\n",
    "\n",
    "                outputs = []\n",
    "                vjs = []\n",
    "                \n",
    "                currentInputs = X_train[idx]\n",
    "                currentOutputs = []\n",
    "                currentVjs = []\n",
    "\n",
    "                for l in range(len(self.layers)):\n",
    "                    currentOutputs = []\n",
    "                    currentVjs = []\n",
    "\n",
    "                    currentLayer = self.layers[l]\n",
    "\n",
    "                    # For each neuron \n",
    "                    for n in range(len(self.layers[l])):\n",
    "                        currentNeuron = self.layers[l][n]                    \n",
    "                        currentOutputs.append(currentNeuron.foward(currentInputs, apply_activation=True, verbose=verbose))                        \n",
    "                        currentVjs.append(currentNeuron.foward(currentInputs, apply_activation=False, verbose=False))\n",
    "\n",
    "                    # For the next layer, the input become the output of the preview layer\n",
    "                    currentInputs = currentOutputs\n",
    "                    outputs.append(currentOutputs)\n",
    "                    vjs.append(currentVjs)\n",
    "                \n",
    "                \n",
    "                # Backwards pass ##########\n",
    "                \n",
    "                lastLayerGradients = []\n",
    "                \n",
    "                # For each layer (reverse order)\n",
    "                for l in range(len(self.layers)-1, -1, -1):\n",
    "                    \n",
    "                    currentLayerGradients = []\n",
    "                    \n",
    "                    # For each neuron\n",
    "                    for n in range(len(self.layers[l])):\n",
    "                        \n",
    "                        currentNeuron = self.layers[l][n] \n",
    "                        \n",
    "                        # Error for the last layer\n",
    "                        if l == len(self.layers) - 1:                            \n",
    "\n",
    "                            derivative = self.activation_function_derivative(currentNeuron.activation, vjs[l][n])\n",
    "                            grad = (outputs[l][n] - y_train[idx][n]) * derivative                  \n",
    "                                                        \n",
    "                        else:\n",
    "                            \n",
    "                            # Lets try to calculated the weighted sum of the gradients and weights \n",
    "                            wSum = 0\n",
    "                            for nextNeuronIdx in range(len(self.layers[l+1])):                                                               \n",
    "                                wkj = self.layers[l+1][nextNeuronIdx].weights[n]\n",
    "                                gradj = lastLayerGradients[nextNeuronIdx]                                \n",
    "                                wSum += wkj*gradj\n",
    "                                                        \n",
    "                            vjDerivative = self.activation_function_derivative(currentNeuron.activation, vjs[l][n])                            \n",
    "                            grad = wSum * vjDerivative\n",
    "                             \n",
    "                        currentLayerGradients.append(grad)\n",
    "                        \n",
    "                        for w in range(len(currentNeuron.weights)):\n",
    "\n",
    "                            if l == 0:\n",
    "                                delta = self.lr * (-grad) * X_train[idx][w]\n",
    "                            else:\n",
    "                                delta = self.lr * (-grad) * outputs[l-1][w]\n",
    "\n",
    "                            currentNeuron.newWeights[w] = currentNeuron.weights[w] + delta \n",
    "                        \n",
    "                    lastLayerGradients = currentLayerGradients\n",
    "                \n",
    "                \n",
    "                ## Now lest update all weights of the network at once\n",
    "                # For each layer\n",
    "                for l in range(len(self.layers)):\n",
    "                    \n",
    "                    # For each neuron \n",
    "                    for n in range(len(self.layers[l])):\n",
    "                        self.layers[l][n].adjustWeights()\n",
    "\n",
    "                ##########            \n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NeuralNetwork: \n",
      " {'activation': 'sigmoid',\n",
      " 'activation_last_layer': 'relu',\n",
      " 'architecture': [4,\n",
      "                  1],\n",
      " 'inputs': 2,\n",
      " 'isClassification': False,\n",
      " 'layers': [[{'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.99317655,  0.8303917 ]), 'weights': array([ 0.99317655,  0.8303917 ])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.70684134,  0.96754681]), 'weights': array([ 0.70684134,  0.96754681])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.00280385,  0.65521076]), 'weights': array([ 0.00280385,  0.65521076])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.01753761,  0.85467606]), 'weights': array([ 0.01753761,  0.85467606])}],\n",
      "            [{'activation': 'relu', 'bias': 1, 'inputs': 4, 'newWeights': array([ 0.70549557,  0.57748402,  0.30325809,  0.6246671 ]), 'weights': array([ 0.70549557,  0.57748402,  0.30325809,  0.6246671 ])}]],\n",
      " 'lr': 0.1,\n",
      " 'momentum': 0} \n",
      "\n",
      "Input:  [2, 3]  -> Vj:  5.47752820545  -> Output:  0.995837748575\n",
      "Input:  [2, 3]  -> Vj:  5.31632312441  -> Output:  0.995113220995\n",
      "Input:  [2, 3]  -> Vj:  2.97123999033  -> Output:  0.951257803065\n",
      "Input:  [2, 3]  -> Vj:  3.59910340031  -> Output:  0.973379783966\n",
      "Input:  [0.99583774857491669, 0.9951132209952398, 0.95125780306460073, 0.97337978396636871]  -> Vj:  3.17373604486  -> Output:  3.17373604486\n",
      "Input:  [2, 3]  -> Vj:  5.54260813153  -> Output:  0.996098978285\n",
      "Input:  [2, 3]  -> Vj:  5.38615115618  -> Output:  0.995441311322\n",
      "Input:  [2, 3]  -> Vj:  3.48733296809  -> Output:  0.970325197001\n",
      "Input:  [2, 3]  -> Vj:  3.97921715432  -> Output:  0.981643007731\n",
      "Input:  [0.99609897828549465, 0.99544131132173774, 0.97032519700088327, 0.98164300773058266]  -> Vj:  6.28405113028  -> Output:  6.28405113028\n",
      "Input:  [2, 3]  -> Vj:  5.59213678864  -> Output:  0.996286783942\n",
      "Input:  [2, 3]  -> Vj:  5.44025290926  -> Output:  0.995680354376\n",
      "Input:  [2, 3]  -> Vj:  3.77141145941  -> Output:  0.977498426882\n",
      "Input:  [2, 3]  -> Vj:  4.19669867515  -> Output:  0.98517783796\n",
      "Input:  [0.996286783941707, 0.99568035437571634, 0.97749842688204747, 0.98517783795986791]  -> Vj:  8.21407490733  -> Output:  8.21407490733\n",
      "Input:  [2, 3]  -> Vj:  5.62504926077  -> Output:  0.996406573802\n",
      "Input:  [2, 3]  -> Vj:  5.47636192551  -> Output:  0.995832911633\n",
      "Input:  [2, 3]  -> Vj:  3.92813690348  -> Output:  0.980699534321\n",
      "Input:  [2, 3]  -> Vj:  4.32041741786  -> Output:  0.986880087396\n",
      "Input:  [0.99640657380175413, 0.9958329116329987, 0.98069953432073209, 0.98688008739623978]  -> Vj:  9.39178117666  -> Output:  9.39178117666\n",
      "Input:  [2, 3]  -> Vj:  5.64585619982  -> Output:  0.996480309078\n",
      "Input:  [2, 3]  -> Vj:  5.4992163191  -> Output:  0.995926684342\n",
      "Input:  [2, 3]  -> Vj:  4.0177015328  -> Output:  0.982323793845\n",
      "Input:  [2, 3]  -> Vj:  4.39228063946  -> Output:  0.987778727593\n",
      "Input:  [0.99648030907780905, 0.99592668434242393, 0.98232379384533575, 0.98777872759305385]  -> Vj:  10.1064348641  -> Output:  10.1064348641\n",
      "Input:  [2, 3]  -> Vj:  5.65872576027  -> Output:  0.996525159333\n",
      "Input:  [2, 3]  -> Vj:  5.51335642831  -> Output:  0.995983646466\n",
      "Input:  [2, 3]  -> Vj:  4.07004340746  -> Output:  0.9832100685\n",
      "Input:  [2, 3]  -> Vj:  4.43465555304  -> Output:  0.988279841071\n",
      "Input:  [0.9965251593329143, 0.99598364646582516, 0.98321006849987436, 0.98827984107098621]  -> Vj:  10.5390869275  -> Output:  10.5390869275\n",
      "Input:  [2, 3]  -> Vj:  5.66660086234  -> Output:  0.996552322616\n",
      "Input:  [2, 3]  -> Vj:  5.52200943808  -> Output:  0.996018112296\n",
      "Input:  [2, 3]  -> Vj:  4.10103971641  -> Output:  0.983714165895\n",
      "Input:  [2, 3]  -> Vj:  4.45987787372  -> Output:  0.988568416819\n",
      "Input:  [0.99655232261593152, 0.99601811229587511, 0.98371416589506122, 0.98856841681878616]  -> Vj:  10.8007146848  -> Output:  10.8007146848\n",
      "Input:  [2, 3]  -> Vj:  5.67139244297  -> Output:  0.996568746377\n",
      "Input:  [2, 3]  -> Vj:  5.52727427493  -> Output:  0.996038938374\n",
      "Input:  [2, 3]  -> Vj:  4.11953960055  -> Output:  0.984007908268\n",
      "Input:  [2, 3]  -> Vj:  4.47497642852  -> Output:  0.988737791468\n",
      "Input:  [0.99656874637707749, 0.99603893837426183, 0.98400790826762707, 0.98873779146845897]  -> Vj:  10.9588258131  -> Output:  10.9588258131\n",
      "Input:  [2, 3]  -> Vj:  5.67429865585  -> Output:  0.996578669786\n",
      "Input:  [2, 3]  -> Vj:  5.53046746206  -> Output:  0.99605151675\n",
      "Input:  [2, 3]  -> Vj:  4.1306326196  -> Output:  0.984181537823\n",
      "Input:  [2, 3]  -> Vj:  4.48404584607  -> Output:  0.988838336438\n",
      "Input:  [0.99657866978643084, 0.99605151674998793, 0.98418153782296325, 0.98883833643781816]  -> Vj:  11.0543451534  -> Output:  11.0543451534\n",
      "Input:  [2, 3]  -> Vj:  5.67605814824  -> Output:  0.996584663757\n",
      "Input:  [2, 3]  -> Vj:  5.53240066132  -> Output:  0.996059112529\n",
      "Input:  [2, 3]  -> Vj:  4.13730283155  -> Output:  0.984285046598\n",
      "Input:  [2, 3]  -> Vj:  4.48950498646  -> Output:  0.988898428897\n",
      "Input:  [0.99658466375652899, 0.99605911252884438, 0.98428504659800864, 0.98889842889743962]  -> Vj:  11.1120395391  -> Output:  11.1120395391\n",
      "\n",
      "NeuralNetwork: \n",
      " {'activation': 'sigmoid',\n",
      " 'activation_last_layer': 'relu',\n",
      " 'architecture': [4,\n",
      "                  1],\n",
      " 'inputs': 2,\n",
      " 'isClassification': False,\n",
      " 'layers': [[{'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 1.02388333,  0.87645187]), 'weights': array([ 1.02388333,  0.87645187])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.74026391,  1.01768066]), 'weights': array([ 0.74026391,  1.01768066])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.18281621,  0.9252293 ]), 'weights': array([ 0.18281621,  0.9252293 ])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.15502865,  1.06091261]), 'weights': array([ 0.15502865,  1.06091261])}],\n",
      "            [{'activation': 'relu', 'bias': 1, 'inputs': 4, 'newWeights': array([ 2.73405473,  2.6047108 ,  2.2713833 ,  2.62087026]), 'weights': array([ 2.73405473,  2.6047108 ,  2.2713833 ,  2.62087026])}]],\n",
      " 'lr': 0.1,\n",
      " 'momentum': 0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(2, [4,1], isClassification=False, lr=0.1, activation='sigmoid',  activation_last_layer='relu')\n",
    "print(\"\\nNeuralNetwork: \\n\", nn, \"\\n\")\n",
    "nn.fit(x=[[2,3]], y=[[11.2]], epochs=10, verbose=True)\n",
    "print(\"\\nNeuralNetwork: \\n\", nn, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e75240630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.draw(size=(10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NeuralNetwork: \n",
      " {'activation': 'sigmoid',\n",
      " 'activation_last_layer': 'sigmoid',\n",
      " 'architecture': [8,\n",
      "                  8],\n",
      " 'inputs': 2,\n",
      " 'isClassification': True,\n",
      " 'layers': [[{'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.08467578,  0.13506452]), 'weights': array([ 0.08467578,  0.13506452])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.90320553,  0.45189299]), 'weights': array([ 0.90320553,  0.45189299])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.15419767,  0.29581103]), 'weights': array([ 0.15419767,  0.29581103])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.58751303,  0.91772038]), 'weights': array([ 0.58751303,  0.91772038])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.11730169,  0.8809078 ]), 'weights': array([ 0.11730169,  0.8809078 ])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.32778591,  0.88793706]), 'weights': array([ 0.32778591,  0.88793706])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.2388278 ,  0.67986429]), 'weights': array([ 0.2388278 ,  0.67986429])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 2, 'newWeights': array([ 0.94412406,  0.2350751 ]), 'weights': array([ 0.94412406,  0.2350751 ])}],\n",
      "            [{'activation': 'sigmoid', 'bias': 1, 'inputs': 8, 'newWeights': array([ 0.4415679 ,  0.15354559,  0.93605385,  0.47824385,  0.69274086,\n",
      "        0.38891837,  0.70794633,  0.78453215]), 'weights': array([ 0.4415679 ,  0.15354559,  0.93605385,  0.47824385,  0.69274086,\n",
      "        0.38891837,  0.70794633,  0.78453215])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 8, 'newWeights': array([ 0.33781304,  0.99892094,  0.25443994,  0.24246946,  0.92203016,\n",
      "        0.38224446,  0.86882161,  0.90773832]), 'weights': array([ 0.33781304,  0.99892094,  0.25443994,  0.24246946,  0.92203016,\n",
      "        0.38224446,  0.86882161,  0.90773832])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 8, 'newWeights': array([ 0.56792963,  0.72325254,  0.89167377,  0.33721959,  0.71736777,\n",
      "        0.50490224,  0.55013874,  0.88627636]), 'weights': array([ 0.56792963,  0.72325254,  0.89167377,  0.33721959,  0.71736777,\n",
      "        0.50490224,  0.55013874,  0.88627636])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 8, 'newWeights': array([ 0.93891157,  0.73368478,  0.25521981,  0.18364476,  0.81609647,\n",
      "        0.66568263,  0.24975697,  0.19425164]), 'weights': array([ 0.93891157,  0.73368478,  0.25521981,  0.18364476,  0.81609647,\n",
      "        0.66568263,  0.24975697,  0.19425164])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 8, 'newWeights': array([ 0.0386165 ,  0.69653363,  0.87226379,  0.16572333,  0.40163007,\n",
      "        0.43040337,  0.24947172,  0.35124522]), 'weights': array([ 0.0386165 ,  0.69653363,  0.87226379,  0.16572333,  0.40163007,\n",
      "        0.43040337,  0.24947172,  0.35124522])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 8, 'newWeights': array([ 0.40105353,  0.95820016,  0.34521777,  0.86829554,  0.15924804,\n",
      "        0.83841466,  0.24785561,  0.49561474]), 'weights': array([ 0.40105353,  0.95820016,  0.34521777,  0.86829554,  0.15924804,\n",
      "        0.83841466,  0.24785561,  0.49561474])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 8, 'newWeights': array([ 0.116503  ,  0.03605281,  0.31450793,  0.86525874,  0.70034006,\n",
      "        0.84387805,  0.54837376,  0.91653326]), 'weights': array([ 0.116503  ,  0.03605281,  0.31450793,  0.86525874,  0.70034006,\n",
      "        0.84387805,  0.54837376,  0.91653326])},\n",
      "             {'activation': 'sigmoid', 'bias': 1, 'inputs': 8, 'newWeights': array([ 0.76905801,  0.52827791,  0.85569391,  0.98782858,  0.65958574,\n",
      "        0.06313372,  0.04787994,  0.27142277]), 'weights': array([ 0.76905801,  0.52827791,  0.85569391,  0.98782858,  0.65958574,\n",
      "        0.06313372,  0.04787994,  0.27142277])}]],\n",
      " 'lr': 0.2,\n",
      " 'momentum': 0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#x, y = criaDataset(400, 0.1)\n",
    "x, y = generateDataset_Q4(1000)\n",
    "#print(x,y)\n",
    "        \n",
    "#x, y = generateDataset_Q3A(400)\n",
    "#x, y = generateDataset_Q3B(400)\n",
    "#print(x,y)\n",
    "nn = NeuralNetwork(2, [8,8], isClassification=True, lr=0.2, activation='sigmoid',  activation_last_layer='sigmoid')\n",
    "print(\"\\nNeuralNetwork: \\n\", nn, \"\\n\")\n",
    "\n",
    "#print(\"Prediction Test: \\n\", nn.predict([2,5,2,3,4], verbose=True), \"\\n\")\n",
    "\n",
    "#nn.save()\n",
    "\n",
    "#print(\"Prediction Test with saved network: \\n\", NeuralNetwork().load().predict([2,5,2,3,4], verbose=False), \"\\n\")\n",
    "\n",
    "nn.fit(x=x, y=y, epochs=10000, verbose=False)\n",
    "\n",
    "# Preenche o array de saidas preditas\n",
    "\n",
    "# y_target = []\n",
    "# for j in range(len(x)):\n",
    "#     y_target.append(y[j])\n",
    "\n",
    "y_pred = []\n",
    "for j in range(len(x)):\n",
    "    y_pred.append(nn.predict(x[j]))\n",
    "\n",
    "#print (\"\\n\\n\", y)\n",
    "#print (\"\\n\\n\", y_pred)\n",
    "\n",
    "print(\"\\nNeuralNetwork: \\n\", nn, \"\\n\")\n",
    "\n",
    "    \n",
    "# # Métricas de Avaliação\n",
    "# print('Matriz de Confusão:')\n",
    "# print(confusion_matrix(y_target, y_pred))\n",
    "# print('F1 Score:')\n",
    "# print(classification_report(y_target, y_pred))\n",
    "\n",
    "\n",
    "# #nn.fit(x=[[2,3]], y=[[17.2]], epochs=4, verbose=True)\n",
    "\n",
    "# print(\"\\nNeuralNetwork: \\n\", nn, \"\\n\")\n",
    "\n",
    "# #print(\"\\nPrediction: \\n\",nn.predict([2,3], verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(np.all(np.equal(y, y_pred), axis=1)) / float(len(y)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import spline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pylab\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # The layers of the network\n",
    "    layers = []\n",
    "    \n",
    "    def __init__(self, inputs, architecture=[1], lr = 0.01, momentum = 0):\n",
    "        \"\"\"\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        # Make sure the architecture is a list and not empty\n",
    "        assert isinstance(architecture, list) and len(architecture) > 0\n",
    "        \n",
    "        # Save the local variables\n",
    "        self.architecture = architecture\n",
    "        self.inputs = (int) inputs\n",
    "        self.momentum = momentum if momentum > 0 else 0\n",
    "        self.lr = lr if lr > 0 else 0.00001 #make sure the LR is not too close to zero\n",
    "\n",
    "        # Create the layers and neurons based on the architecture\n",
    "        self.initLayers()\n",
    "        \n",
    "    def initLayers(self)\n",
    "        \"\"\"\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        # The amount of inputs of each Neuron need to be the total of neurons of the previus layer OR self.inputs if this is the first layer\n",
    "        totalInputs = self.inputs\n",
    "        \n",
    "        # Clear all layers (and weigths)\n",
    "        layers = []\n",
    "        \n",
    "        # For each layer of the architecture\n",
    "        for n in architecture:  \n",
    "            \n",
    "            # Start an array with the current layer\n",
    "            currentLayer = []\n",
    "            \n",
    "            # Create n neurons \n",
    "            for count in range(n):\n",
    "                currentLayer.append( Neuron(totalInputs, activation='sigmoid') )\n",
    "                \n",
    "            totalInputs = n\n",
    "            \n",
    "            # Add the current layer to the layer list\n",
    "            layers.append(currentLayer);\n",
    "                \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Converte a classe atual em uma string fácil de entender\n",
    "        \"\"\"\n",
    "        return str(self.__class__) + \": \" + str(self.__dict__)    \n",
    "        \n",
    "        \n",
    "        \n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, inputs, lr = 0.01):\n",
    "        \n",
    "        # Os inputspode ser um array de neuros ou um escalar\n",
    "        if (isinstance(inputs, list)):\n",
    "            self.weights = np.random.rand(len(inputs))\n",
    "        else:\n",
    "            self.weights = np.random.rand(inputs)\n",
    "        \n",
    "        self.bias = 1\n",
    "        self.inputs = inputs\n",
    "        self.lr = lr if lr > 0 else 0.0001\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.__class__) + \": \" + str(self.__dict__)\n",
    "\n",
    "\n",
    "    def vj(self, x) :\n",
    "        if (isinstance(self.inputs, list)):            \n",
    "            signals = []            \n",
    "            for i in range(len(self.inputs)):\n",
    "                signals.append( self.inputs[i].forward(x))             \n",
    "            return np.dot(self.weights, signals) + self.bias          \n",
    "        else:\n",
    "            return np.dot(self.weights, x) + self.bias\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activation( self.vj(x) )\n",
    "\n",
    "\n",
    "    def backward(self, x, y, soma=None): \n",
    "\n",
    "        somaPonderada = 0\n",
    "        \n",
    "        y_pred = self.forward(x)\n",
    "        \n",
    "        for idx in range(len(self.weights)):\n",
    "\n",
    "            if (soma == None): \n",
    "                error = y - y_pred\n",
    "                gradient = error * self.activation_derivative( self.vj(x) )\n",
    "            else:\n",
    "                gradient = self.activation_derivative( self.vj(x) ) * soma\n",
    "       \n",
    "            somaPonderada = somaPonderada + gradient*self.weights[idx]\n",
    "            \n",
    "            self.weights[idx] = self.weights[idx] + self.lr * x[idx] *  gradient\n",
    "            \n",
    "        if (isinstance(self.inputs, list)): \n",
    "            for i in range(len(self.inputs)):\n",
    "                self.inputs[i].backward(x, y, soma=somaPonderada)\n",
    "            \n",
    "    def activation(self, x):\n",
    "        return x\n",
    "\n",
    "    def activation_derivative(self, x):\n",
    "        return 1\n",
    "        \n",
    "    def adjustWeights(self, x, error):            \n",
    "        self.bias = self.bias + error*self.lr        \n",
    "        for idx in range(len(self.weights)):\n",
    "            self.weights[idx] = self.weights[idx] + error*self.lr*x[idx]\n",
    "\n",
    "def predict( out):\n",
    "        \"\"\"\n",
    "        Função que discretiza a saída\n",
    "        Parâmetro: y - saída do neurònio\n",
    "        \"\"\"\n",
    "        if out < 0:\n",
    "            return 0\n",
    "        elif out > 7:\n",
    "            return 7\n",
    "        else:\n",
    "            return round(out)\n",
    "        \n",
    "# TESTES\n",
    "X, Y = criaDataset(50, 0.1)   \n",
    "\n",
    "n_inputs = len(X[0])\n",
    "        \n",
    "layer1 = [ Neuron(n_inputs, lr=0.1) , Neuron(n_inputs, lr=0.1)]\n",
    "    \n",
    "layer2 = [ Neuron(layer1, lr=0.1)] \n",
    "\n",
    "network = layer2\n",
    "\n",
    "#y_pred = network[0].forward(X[0])\n",
    "#print(y_pred)\n",
    "\n",
    "perceptron = network[0]\n",
    "\n",
    "# perceptron = Neuron(len(X[0]))\n",
    "\n",
    "print(perceptron)\n",
    "\n",
    "for epoch in range(10):\n",
    "    erros_interacao = []\n",
    "    for i in range(len(X)):\n",
    "        y_pred = predict( perceptron.forward(X[i]) )\n",
    "        error = Y[i] - y_pred\n",
    "        erros_interacao.append(error*error)\n",
    "        perceptron.backward(X[i], Y[i])\n",
    "    erro_medio = np.average(erros_interacao)\n",
    "    print(erro_medio)\n",
    "\n",
    "# Preenche o array de saidas preditas\n",
    "y_pred = []\n",
    "for j in range(len(X)):    \n",
    "    y_pred.append( predict( perceptron.forward(X[j]) ))\n",
    "#print(Y, y_pred)\n",
    "# Métricas de Avaliação\n",
    "print('Matriz de Confusão:')\n",
    "print(confusion_matrix(Y, y_pred))\n",
    "print('F1 Score:')\n",
    "print(classification_report(Y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação do Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import spline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(num_inputs): \n",
    "    \"\"\"\n",
    "    Função que inicializa os pesos e bias aleatoriamente utilizando numpy\n",
    "    Parâmetro: num_inputs - quantidade de entradas X\n",
    "    Retorna: w,b - pesos e bias da rede inicializados\n",
    "    \"\"\"\n",
    "    w = np.random.rand(num_inputs) - 0.5\n",
    "    b =  np.random.random() - 0.5       \n",
    "    return w,b\n",
    "\n",
    "# Teste da função weight_init:\n",
    "print(weight_init(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_func(func_type, z):\n",
    "    \"\"\"\n",
    "    Função que implementa as funções de ativação mais comuns\n",
    "    Parãmetros: func_type - uma string que contém a função de ativação desejada\n",
    "                z - vetor com os valores de entrada X multiplicado pelos pesos\n",
    "    Retorna: saída da função de ativação\n",
    "    \"\"\"\n",
    "    if func_type == 'sigmoid':\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    elif func_type == 'tanh':\n",
    "        return math.sinh(z)/math.cosh(z)\n",
    "    elif func_type == 'relu':\n",
    "        return 0 if (z<0) else z\n",
    "    elif func_type == 'linear':\n",
    "        return z\n",
    "    elif func_type == 'degrau':\n",
    "        return 0 if (z < 0) else  1\n",
    "    \n",
    "def visualizeActivationFunc(z, func_type):\n",
    "    pylab.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "    z = np.arange(-5., 5., 0.2)\n",
    "    func = []\n",
    "    for i in range(len(z)):\n",
    "        func.append(activation_func(func_type, z[i]))\n",
    "    \n",
    "    pylab.rcParams['figure.figsize'] = (5.0, 2.0)\n",
    "    plt.plot(z,func)\n",
    "    plt.title(\"Função: \"+func_type)\n",
    "    plt.xlabel('Entrada')\n",
    "    plt.ylabel('Valores de Saída')\n",
    "    plt.show()\n",
    "\n",
    "# Testa as funções de ativação\n",
    "visualizeActivationFunc(np.arange(-5., 5., 0.2), 'degrau')\n",
    "visualizeActivationFunc(np.arange(-5., 5., 0.2), 'sigmoid')\n",
    "visualizeActivationFunc(np.arange(-5., 5., 0.2), 'tanh')\n",
    "visualizeActivationFunc(np.arange(-5., 5., 0.2), 'relu')\n",
    "visualizeActivationFunc(np.arange(-5., 5., 0.2), 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(w,b,X):\n",
    "    \"\"\"\n",
    "    Função que implementa a etapa forward propagate do neurônio\n",
    "    Parâmetros: w - pesos\n",
    "                b - bias\n",
    "                X - entradas\n",
    "    \"\"\"\n",
    "    z = np.dot(w,X)+b\n",
    "    out = activation_func('relu', z)\n",
    "    return out\n",
    "\n",
    "# Teste: forward\n",
    "x = (1, 1, 1)\n",
    "w,b = weight_init(3)\n",
    "print(\"X: \", x)\n",
    "print(\"Pesos: \", w)\n",
    "print(\"Bias: \", b)\n",
    "print(\"Y: \", forward(w,b,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(out):\n",
    "    \"\"\"\n",
    "    Função que discretiza a saída\n",
    "    Parâmetro: y - saída do neurònio\n",
    "    \"\"\"\n",
    "    if out < 0:\n",
    "        return 0\n",
    "    elif out > 7:\n",
    "        return 7\n",
    "    else:\n",
    "        return round(out)\n",
    "\n",
    "# Teste: predict\n",
    "print (\"Entrada=1.1 -> Saida=\", predict(1.1))\n",
    "print (\"Entrada=4.5 -> Saida=\", predict(4.5))\n",
    "print (\"Entrada=70.0 -> Saida=\", predict(70.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x, y, num_interaction, learning_rate):\n",
    "    \"\"\"\n",
    "    Função que implementa o loop do treinamento \n",
    "    Parâmetros: x - entrada da rede \n",
    "                y - rótulos/labels\n",
    "                num_interaction - quantidade de interações desejada para a rede convergir\n",
    "                learning_rate - taxa de aprendizado para cálculo do erro\n",
    "    \"\"\"\n",
    "    training_interation = []\n",
    "    training_erro = []\n",
    "    \n",
    "    #Passo 1 - Inicie os pesos e bias (~1 linha)\n",
    "    w,b = weight_init(len(x[0]))\n",
    "    #Passo 2 - Loop por X interações\n",
    "    for i in range(num_interaction):\n",
    "        \n",
    "        # Ajuda no calculo do erro médio quadrado\n",
    "        erros_interacao = []\n",
    "        \n",
    "        for j in range(len(x)): # para cada exemplo\n",
    "            \n",
    "            # Passo 3 -  calcule a saída do neurônio (~1 linha)\n",
    "            y_calc = predict(forward(w,b,x[j]))\n",
    "            \n",
    "            # Passo 4 - calcule o erro entre a saída obtida e a saída desejada nos rótulos/labels (~1 linha)\n",
    "            erro = y[j] - y_calc  \n",
    "            \n",
    "            # Adiciona o erro quadrado dessa instancia\n",
    "            erros_interacao.append(erro*erro)\n",
    "            \n",
    "            # Ajusta os pesos e bias\n",
    "            for idx in range(len(w)):\n",
    "                w[idx] = w[idx] + erro*learning_rate*x[j][idx]            \n",
    "\n",
    "            b = b + erro*learning_rate\n",
    "        \n",
    "        # Calcula o erro médio dessa interação\n",
    "        erro_medio = np.average(erros_interacao)\n",
    "        \n",
    "        training_interation.append(i)\n",
    "        training_erro.append(erro_medio) \n",
    "        \n",
    "        # Apenas para garantir que pare quando o erro atingir um valor limite mínimo\n",
    "        if (erro_medio < 0.00001):\n",
    "            print(\"Finalizado na interação %d visto que o erro médio já se tornou muito pequeno %f\" % (i, erro_medio))\n",
    "            break;\n",
    "    \n",
    "    # Cria um grafico com os dados sobre o erro de treinamento\n",
    "    pylab.rcParams['figure.figsize'] = (15.0, 10.0)\n",
    "    plt.plot(training_interation, training_erro)\n",
    "    plt.xlabel('Interação')\n",
    "    plt.ylabel('Erro médio quadrado')\n",
    "    plt.show()\n",
    "    \n",
    "    return w,b\n",
    "\n",
    "def validar(w,b,x,y):\n",
    "\n",
    "    # Preenche o array de saidas preditas\n",
    "    y_pred = []\n",
    "    for j in range(len(x)):\n",
    "        y_pred.append(predict(forward(w,b,x[j])))\n",
    "       \n",
    "    # Métricas de Avaliação\n",
    "    print('Matriz de Confusão:')\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    print('F1 Score:')\n",
    "    print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação dos Datasets de Treinamento e Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = criaDataset(800, 0.1)\n",
    "X_val, Y_val = criaDataset(200, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainamento do Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = perceptron(X_train, Y_train, 1000, 0.001)\n",
    "print(\"Pesos Aprendidos:\", w, \"\\nBias Aprendido:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjunto de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validar(w, b, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjunto de Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validar(w, b, X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
