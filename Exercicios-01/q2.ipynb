{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ª Lista de Exercícios - Deep Learning - Questão 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Questão 1](imgs/questoes/q2.png \"Questão 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "from viznet import connecta2a, connect121, node_sequence, NodeBrush, EdgeBrush, DynamicShow\n",
    "import pickle\n",
    "import pdb\n",
    "pdb.set_trace = lambda: 1\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"\n",
    "    Class that represents a neuron inside the neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs, activation='sigmoid'):   \n",
    "        \"\"\"\n",
    "        Create a neuron object.\n",
    "        Param:\n",
    "            - inputs: ...\n",
    "            - activation: ...\n",
    "        \"\"\"\n",
    "        \n",
    "        self.weights = np.random.rand(inputs)\n",
    "        self.bias = np.random.random()\n",
    "        \n",
    "        assert inputs > 0\n",
    "        self.inputs = int(inputs)\n",
    "        \n",
    "        assert activation == 'sigmoid' or activation == 'tanh' or activation == 'relu' or activation == 'linear' or  activation == 'step'\n",
    "        self.activation = activation\n",
    "\n",
    "    def activation_function(self, func_type, value):\n",
    "        \"\"\"\n",
    "        Most commons activation functions.\n",
    "        Param: func_type - A tring with one of the valid values: sigmoid, tanh, relu, linear and step\n",
    "                    value - Apply the function to the value\n",
    "        Return: The result of the activation function\n",
    "        \"\"\"\n",
    "        \n",
    "        assert func_type == 'sigmoid' or func_type == 'tanh' or func_type == 'relu' or func_type == 'linear' or  func_type == 'step'\n",
    "        \n",
    "        if func_type == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-value))\n",
    "        elif func_type == 'tanh':\n",
    "            return math.sinh(value) / math.cosh(value)\n",
    "        elif func_type == 'relu':\n",
    "            return 0 if (value < 0) else value\n",
    "        elif func_type == 'linear':\n",
    "            return value\n",
    "        elif func_type == 'step':\n",
    "            return 0 if (value < 0) else 1\n",
    "            \n",
    "    def foward(self, x, apply_activation=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Calculate the output of the neuron. \n",
    "        Param:\n",
    "            - inputs: Array with the inputs signals\n",
    "            - apply_activation: If true, Apply the activation final. If false, return the output without activation function (Vj)\n",
    "            - verbose: If true, show some data on the stdout\n",
    "        Return: The neuron output (with or without the activation function applied)\n",
    "        \"\"\"\n",
    "        vj = np.dot(x, self.weights) + self.bias\n",
    "        out = vj if (not apply_activation) else self.activation_function(self.activation , vj)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Input: \", x, \" -> Vj: \", vj, \" -> Output: \", out)\n",
    "            \n",
    "        return out\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Alternative way to see the object as string\n",
    "        \"\"\"\n",
    "        return pprint.pformat(vars(self), indent=0, width=1000, depth=None)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Class that represents a Feed-foward Neural Network that can be trained using backpropagation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inputs, architecture=[1], lr=0.01, momentum=0, outputFilter=None):\n",
    "        \"\"\"\n",
    "        Create a feed-foward neural network object.\n",
    "        Param:\n",
    "            - inputs: ...\n",
    "            - architecture: ...\n",
    "            - lr: ...\n",
    "            - momentum: ...\n",
    "            - outputFilter: Function that is applyed in the end of the calculation. Helps on classification tasks.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Make sure the architecture is a list and not empty\n",
    "        assert isinstance(architecture, list) and len(architecture) > 0\n",
    "        \n",
    "        self.architecture = architecture        \n",
    "        assert inputs > 0\n",
    "        self.inputs = int(inputs)\n",
    "        self.momentum = momentum if momentum > 0 else 0\n",
    "        self.lr = lr if lr > 0 else 0.00001 # Make sure the LR is not too close to zero\n",
    "        self.outputFilter = outputFilter\n",
    "\n",
    "        # Create the layers and neurons based on the architecture\n",
    "        self.initLayers()\n",
    "        \n",
    "    def initLayers(self):\n",
    "        \"\"\"\n",
    "        Initialize the layers and create the neurons\n",
    "        \"\"\"\n",
    "        \n",
    "        # The amount of inputs of each Neuron need to be the total of neurons of the previus layer OR self.inputs if this is the first layer\n",
    "        totalInputs = self.inputs\n",
    "        \n",
    "        # Clear all layers (and weigths)\n",
    "        self.layers = []\n",
    "        \n",
    "        # For each layer of the architecture\n",
    "        for n in self.architecture:  \n",
    "            \n",
    "            # Start an array with the current layer\n",
    "            currentLayer = []\n",
    "            \n",
    "            # Create n neurons \n",
    "            for count in range(n):\n",
    "                currentLayer.append( Neuron(totalInputs, activation='linear'))\n",
    "                \n",
    "            totalInputs = n\n",
    "            \n",
    "            # Add the current layer to the layer list\n",
    "            self.layers.append(currentLayer)                \n",
    "     \n",
    "    def evaluate(self, x=None, y=None):\n",
    "        \"\"\"\n",
    "        Returns the loss value for each data.\n",
    "        Param:\n",
    "            x: The input samples data, as a Numpy array (or list of Numpy arrays).\n",
    "            y: The target values\n",
    "        Return: An array of loss values \n",
    "        \"\"\"\n",
    "        # TODO \n",
    "        return None    \n",
    "        \n",
    "        \n",
    "    def fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=True, validation_split=0.0, shuffle=True):\n",
    "        \"\"\"\n",
    "        Train the Neural Network\n",
    "        Param:\n",
    "            x: The input samples data, as a Numpy array (or list of Numpy arrays).\n",
    "        Return: Numpy array(s) of predictions.\n",
    "        \"\"\"\n",
    "        # TODO \n",
    "        return None\n",
    "    \n",
    "    def predict(self, x, verbose=False):\n",
    "        \"\"\"\n",
    "        Create the predictions for the input samples.\n",
    "        Param:\n",
    "            x: One input data\n",
    "            verbose: If true, show some data on the stdout\n",
    "        Return: the predicted value\n",
    "        \"\"\"\n",
    "\n",
    "        # On the first layer, the input is the data row\n",
    "        currentInputs = x\n",
    "\n",
    "        currentOutputs = []\n",
    "\n",
    "        # For each layer\n",
    "        for l in range(len(self.layers)):\n",
    "            currentOutputs = []\n",
    "            \n",
    "            currentLayer = self.layers[l]\n",
    "\n",
    "            # For each neuron \n",
    "            for n in range(len(self.layers[l])):\n",
    "                currentNeuron = self.layers[l][n]                    \n",
    "                currentOutputs.append(currentNeuron.foward(currentInputs, apply_activation=True, verbose=verbose))\n",
    "\n",
    "            # For the next layer, the input become the output of the preview layer\n",
    "            currentInputs = currentOutputs\n",
    "        \n",
    "        # Apply the output filter if needed\n",
    "        for i in range(len(currentOutputs)):\n",
    "            currentOutputs[i] = currentOutputs[i] if self.outputFilter == None else self.outputFilter(currentOutputs[i])\n",
    "            \n",
    "        return currentOutputs\n",
    "    \n",
    "    def save(self, file='neuralnet.pkl'):\n",
    "        \"\"\"\n",
    "        Serialize the Neural Network to a file that can be loaded later.\n",
    "        Param:\n",
    "            file: The file path where the object will be saved\n",
    "        \"\"\"\n",
    "        assert file != None\n",
    "        with open(file, 'wb') as output:  # Overwrites any existing file.\n",
    "            pickle.dump(self, output, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def load(self, file='neuralnet.pkl'):\n",
    "        \"\"\"\n",
    "        Serialize the Neural Network to a file that can be loaded later.\n",
    "        Param:\n",
    "            file: The file path where the object will be saved\n",
    "        \"\"\"\n",
    "        assert file != None\n",
    "        with open(load, 'rb') as input:\n",
    "            tmp = pickle.load(input)\n",
    "        self.__dict__.update(tmp_dict)\n",
    "        \n",
    "    def draw(self, file=None, size=(10,6)):\n",
    "        \"\"\"\n",
    "        Draw the network architecture \n",
    "        Param:\n",
    "            - file: The file where the image will be saved. Default: None\n",
    "            - size: the image size. Default: (10,6)\n",
    "        \"\"\"\n",
    "        \n",
    "        with DynamicShow(size, filename=file) as d:\n",
    "            \n",
    "            num_hidden_layer = len(self.architecture) - 2\n",
    "            token_list = ['\\sigma^z'] + ['y^{(%s)}' % (i + 1) for i in range(num_hidden_layer)] + ['\\psi']\n",
    "            kind_list = ['nn.input'] + ['nn.hidden'] * num_hidden_layer + ['nn.output']\n",
    "            radius_list = [0.1] + [0.2] * num_hidden_layer + [0.3]\n",
    "            x_list = 1.5 * np.arange(len(self.architecture)) + 1.5\n",
    "\n",
    "            seq_list = []\n",
    "            \n",
    "            # Input pins\n",
    "            inputPins = NodeBrush('qc.C', d.ax)\n",
    "            seq_list.append(node_sequence(inputPins, self.inputs, center=(0, 0), space=(0, 1)))\n",
    "            \n",
    "            # Network and connections\n",
    "            for n, kind, radius, y in zip(self.architecture, kind_list, radius_list, x_list):\n",
    "                b = NodeBrush(kind, d.ax)\n",
    "                seq_list.append(node_sequence(b, n, center=(y, 0), space=(0, 1)))\n",
    "            \n",
    "            for st, et in zip(seq_list[:-1], seq_list[1:]):\n",
    "                connecta2a(st, et, EdgeBrush('-->', d.ax))\n",
    "            \n",
    "            # Output pins\n",
    "            outputEdge = EdgeBrush('---', d.ax)\n",
    "            outputPins = NodeBrush('qc.C', d.ax)\n",
    "            seq_list.append(node_sequence(outputPins, self.architecture[-1], center=(x_list[-1]+1.5, 0), space=(0, 1)))            \n",
    "            connect121( seq_list[-2], seq_list[-1], outputEdge)                \n",
    "                    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Alternative way to see the object as string\n",
    "        \"\"\"\n",
    "        return pprint.pformat(vars(self), indent=1, width=1, depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [2, 5]  -> Vj:  4.98167510496  -> Output:  4.98167510496\n",
      "Input:  [2, 5]  -> Vj:  5.58202139598  -> Output:  5.58202139598\n",
      "Input:  [4.98167510496235, 5.5820213959800595]  -> Vj:  4.69134715116  -> Output:  4.69134715116\n",
      "[5]\n",
      "{'architecture': [2,\n",
      "                  1],\n",
      " 'inputs': 2,\n",
      " 'layers': [[{'activation': 'linear', 'bias': 0.4468622817596245, 'inputs': 2, 'weights': array([ 0.44911454,  0.72731675])},\n",
      "             {'activation': 'linear', 'bias': 0.5057164019286596, 'inputs': 2, 'weights': array([ 0.74444602,  0.71748259])}],\n",
      "            [{'activation': 'linear', 'bias': 0.6887049162739781, 'inputs': 2, 'weights': array([ 0.5018433 ,  0.26918957])}]],\n",
      " 'lr': 0.01,\n",
      " 'momentum': 0,\n",
      " 'outputFilter': <function <lambda> at 0x7feaa14a9620>}\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x7feaa14a9620>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-0b3cf2be88a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-156-4d38c76f08e0>\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Overwrites any existing file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neuralnet.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x7feaa14a9620>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(2, [2, 1], outputFilter=lambda x: int(round(x)))\n",
    "\n",
    "print(nn.predict([2,5], verbose=True))\n",
    "\n",
    "print(nn)\n",
    "nn.save()\n",
    "\n",
    "nn.load()\n",
    "nn.draw()\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feaa14c57b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def randomizeArrayData(arr, variation=0.1):\n",
    "    \"\"\" \n",
    "    Adiciona uma variação aleatoria em uma array de floats \n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = arr[i] + (np.random.random()-0.5)*variation\n",
    "        \n",
    "    return arr\n",
    "\n",
    "def criaDataset(n, variation=0.1):\n",
    "    \"\"\" \n",
    "    Cria um conjunto de exemplos para a questão 1. Classificação de arestas em um cubo \n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    # Crie n/8 exemplos para cada classe\n",
    "    for i in range(math.ceil(n/8)):\n",
    "        X.append(randomizeArrayData([0, 0, 0], variation)) \n",
    "        Y.append(0)\n",
    "        X.append(randomizeArrayData([0, 0, 1], variation)) \n",
    "        Y.append(1)\n",
    "        X.append(randomizeArrayData([0, 1, 0], variation)) \n",
    "        Y.append(2)\n",
    "        X.append(randomizeArrayData([0, 1, 1], variation)) \n",
    "        Y.append(3)\n",
    "        X.append(randomizeArrayData([1, 0, 0], variation)) \n",
    "        Y.append(4)\n",
    "        X.append(randomizeArrayData([1, 0, 1], variation)) \n",
    "        Y.append(5)\n",
    "        X.append(randomizeArrayData([1, 1, 0], variation)) \n",
    "        Y.append(6)\n",
    "        X.append(randomizeArrayData([1, 1, 1], variation)) \n",
    "        Y.append(7)\n",
    "        \n",
    "    return X, Y\n",
    "\n",
    "print(\"Criação de um dataset com 16 pontos e variação de +/- 20%:\\n\")\n",
    "display(criaDataset(16, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import spline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pylab\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # The layers of the network\n",
    "    layers = []\n",
    "    \n",
    "    def __init__(self, inputs, architecture=[1], lr = 0.01, momentum = 0):\n",
    "        \"\"\"\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        # Make sure the architecture is a list and not empty\n",
    "        assert isinstance(architecture, list) and len(architecture) > 0\n",
    "        \n",
    "        # Save the local variables\n",
    "        self.architecture = architecture\n",
    "        self.inputs = (int) inputs\n",
    "        self.momentum = momentum if momentum > 0 else 0\n",
    "        self.lr = lr if lr > 0 else 0.00001 #make sure the LR is not too close to zero\n",
    "\n",
    "        # Create the layers and neurons based on the architecture\n",
    "        self.initLayers()\n",
    "        \n",
    "    def initLayers(self)\n",
    "        \"\"\"\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        # The amount of inputs of each Neuron need to be the total of neurons of the previus layer OR self.inputs if this is the first layer\n",
    "        totalInputs = self.inputs\n",
    "        \n",
    "        # Clear all layers (and weigths)\n",
    "        layers = []\n",
    "        \n",
    "        # For each layer of the architecture\n",
    "        for n in architecture:  \n",
    "            \n",
    "            # Start an array with the current layer\n",
    "            currentLayer = []\n",
    "            \n",
    "            # Create n neurons \n",
    "            for count in range(n):\n",
    "                currentLayer.append( Neuron(totalInputs, activation='sigmoid') )\n",
    "                \n",
    "            totalInputs = n\n",
    "            \n",
    "            # Add the current layer to the layer list\n",
    "            layers.append(currentLayer);\n",
    "                \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Converte a classe atual em uma string fácil de entender\n",
    "        \"\"\"\n",
    "        return str(self.__class__) + \": \" + str(self.__dict__)    \n",
    "        \n",
    "        \n",
    "        \n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, inputs, lr = 0.01):\n",
    "        \n",
    "        # Os inputspode ser um array de neuros ou um escalar\n",
    "        if (isinstance(inputs, list)):\n",
    "            self.weights = np.random.rand(len(inputs))\n",
    "        else:\n",
    "            self.weights = np.random.rand(inputs)\n",
    "        \n",
    "        self.bias = 1\n",
    "        self.inputs = inputs\n",
    "        self.lr = lr if lr > 0 else 0.0001\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.__class__) + \": \" + str(self.__dict__)\n",
    "\n",
    "\n",
    "    def vj(self, x) :\n",
    "        if (isinstance(self.inputs, list)):            \n",
    "            signals = []            \n",
    "            for i in range(len(self.inputs)):\n",
    "                signals.append( self.inputs[i].forward(x))             \n",
    "            return np.dot(self.weights, signals) + self.bias          \n",
    "        else:\n",
    "            return np.dot(self.weights, x) + self.bias\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activation( self.vj(x) )\n",
    "\n",
    "\n",
    "    def backward(self, x, y, soma=None): \n",
    "\n",
    "        somaPonderada = 0\n",
    "        \n",
    "        y_pred = self.forward(x)\n",
    "        \n",
    "        for idx in range(len(self.weights)):\n",
    "\n",
    "            if (soma == None): \n",
    "                error = y - y_pred\n",
    "                gradient = error * self.activation_derivative( self.vj(x) )\n",
    "            else:\n",
    "                gradient = self.activation_derivative( self.vj(x) ) * soma\n",
    "       \n",
    "            somaPonderada = somaPonderada + gradient*self.weights[idx]\n",
    "            \n",
    "            self.weights[idx] = self.weights[idx] + self.lr * x[idx] *  gradient\n",
    "            \n",
    "        if (isinstance(self.inputs, list)): \n",
    "            for i in range(len(self.inputs)):\n",
    "                self.inputs[i].backward(x, y, soma=somaPonderada)\n",
    "            \n",
    "    def activation(self, x):\n",
    "        return x\n",
    "\n",
    "    def activation_derivative(self, x):\n",
    "        return 1\n",
    "        \n",
    "    def adjustWeights(self, x, error):            \n",
    "        self.bias = self.bias + error*self.lr        \n",
    "        for idx in range(len(self.weights)):\n",
    "            self.weights[idx] = self.weights[idx] + error*self.lr*x[idx]\n",
    "\n",
    "def predict( out):\n",
    "        \"\"\"\n",
    "        Função que discretiza a saída\n",
    "        Parâmetro: y - saída do neurònio\n",
    "        \"\"\"\n",
    "        if out < 0:\n",
    "            return 0\n",
    "        elif out > 7:\n",
    "            return 7\n",
    "        else:\n",
    "            return round(out)\n",
    "        \n",
    "# TESTES\n",
    "X, Y = criaDataset(50, 0.1)   \n",
    "\n",
    "n_inputs = len(X[0])\n",
    "        \n",
    "layer1 = [ Neuron(n_inputs, lr=0.1) , Neuron(n_inputs, lr=0.1)]\n",
    "    \n",
    "layer2 = [ Neuron(layer1, lr=0.1)] \n",
    "\n",
    "network = layer2\n",
    "\n",
    "#y_pred = network[0].forward(X[0])\n",
    "#print(y_pred)\n",
    "\n",
    "perceptron = network[0]\n",
    "\n",
    "# perceptron = Neuron(len(X[0]))\n",
    "\n",
    "print(perceptron)\n",
    "\n",
    "for epoch in range(10):\n",
    "    erros_interacao = []\n",
    "    for i in range(len(X)):\n",
    "        y_pred = predict( perceptron.forward(X[i]) )\n",
    "        error = Y[i] - y_pred\n",
    "        erros_interacao.append(error*error)\n",
    "        perceptron.backward(X[i], Y[i])\n",
    "    erro_medio = np.average(erros_interacao)\n",
    "    print(erro_medio)\n",
    "\n",
    "# Preenche o array de saidas preditas\n",
    "y_pred = []\n",
    "for j in range(len(X)):    \n",
    "    y_pred.append( predict( perceptron.forward(X[j]) ))\n",
    "#print(Y, y_pred)\n",
    "# Métricas de Avaliação\n",
    "print('Matriz de Confusão:')\n",
    "print(confusion_matrix(Y, y_pred))\n",
    "print('F1 Score:')\n",
    "print(classification_report(Y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação do Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import spline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(num_inputs): \n",
    "    \"\"\"\n",
    "    Função que inicializa os pesos e bias aleatoriamente utilizando numpy\n",
    "    Parâmetro: num_inputs - quantidade de entradas X\n",
    "    Retorna: w,b - pesos e bias da rede inicializados\n",
    "    \"\"\"\n",
    "    w = np.random.rand(num_inputs) - 0.5\n",
    "    b =  np.random.random() - 0.5       \n",
    "    return w,b\n",
    "\n",
    "# Teste da função weight_init:\n",
    "print(weight_init(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_func(func_type, z):\n",
    "    \"\"\"\n",
    "    Função que implementa as funções de ativação mais comuns\n",
    "    Parãmetros: func_type - uma string que contém a função de ativação desejada\n",
    "                z - vetor com os valores de entrada X multiplicado pelos pesos\n",
    "    Retorna: saída da função de ativação\n",
    "    \"\"\"\n",
    "    if func_type == 'sigmoid':\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    elif func_type == 'tanh':\n",
    "        return math.sinh(z)/math.cosh(z)\n",
    "    elif func_type == 'relu':\n",
    "        return 0 if (z<0) else z\n",
    "    elif func_type == 'linear':\n",
    "        return z\n",
    "    elif func_type == 'degrau':\n",
    "        return 0 if (z < 0) else  1\n",
    "    \n",
    "def visualizeActivationFunc(z, func_type):\n",
    "    pylab.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "    z = np.arange(-5., 5., 0.2)\n",
    "    func = []\n",
    "    for i in range(len(z)):\n",
    "        func.append(activation_func(func_type, z[i]))\n",
    "    \n",
    "    pylab.rcParams['figure.figsize'] = (5.0, 2.0)\n",
    "    plt.plot(z,func)\n",
    "    plt.title(\"Função: \"+func_type)\n",
    "    plt.xlabel('Entrada')\n",
    "    plt.ylabel('Valores de Saída')\n",
    "    plt.show()\n",
    "\n",
    "# Testa as funções de ativação\n",
    "visualizeActivationFunc(np.arange(-5., 5., 0.2), 'degrau')\n",
    "visualizeActivationFunc(np.arange(-5., 5., 0.2), 'sigmoid')\n",
    "visualizeActivationFunc(np.arange(-5., 5., 0.2), 'tanh')\n",
    "visualizeActivationFunc(np.arange(-5., 5., 0.2), 'relu')\n",
    "visualizeActivationFunc(np.arange(-5., 5., 0.2), 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(w,b,X):\n",
    "    \"\"\"\n",
    "    Função que implementa a etapa forward propagate do neurônio\n",
    "    Parâmetros: w - pesos\n",
    "                b - bias\n",
    "                X - entradas\n",
    "    \"\"\"\n",
    "    z = np.dot(w,X)+b\n",
    "    out = activation_func('relu', z)\n",
    "    return out\n",
    "\n",
    "# Teste: forward\n",
    "x = (1, 1, 1)\n",
    "w,b = weight_init(3)\n",
    "print(\"X: \", x)\n",
    "print(\"Pesos: \", w)\n",
    "print(\"Bias: \", b)\n",
    "print(\"Y: \", forward(w,b,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(out):\n",
    "    \"\"\"\n",
    "    Função que discretiza a saída\n",
    "    Parâmetro: y - saída do neurònio\n",
    "    \"\"\"\n",
    "    if out < 0:\n",
    "        return 0\n",
    "    elif out > 7:\n",
    "        return 7\n",
    "    else:\n",
    "        return round(out)\n",
    "\n",
    "# Teste: predict\n",
    "print (\"Entrada=1.1 -> Saida=\", predict(1.1))\n",
    "print (\"Entrada=4.5 -> Saida=\", predict(4.5))\n",
    "print (\"Entrada=70.0 -> Saida=\", predict(70.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x, y, num_interaction, learning_rate):\n",
    "    \"\"\"\n",
    "    Função que implementa o loop do treinamento \n",
    "    Parâmetros: x - entrada da rede \n",
    "                y - rótulos/labels\n",
    "                num_interaction - quantidade de interações desejada para a rede convergir\n",
    "                learning_rate - taxa de aprendizado para cálculo do erro\n",
    "    \"\"\"\n",
    "    training_interation = []\n",
    "    training_erro = []\n",
    "    \n",
    "    #Passo 1 - Inicie os pesos e bias (~1 linha)\n",
    "    w,b = weight_init(len(x[0]))\n",
    "    #Passo 2 - Loop por X interações\n",
    "    for i in range(num_interaction):\n",
    "        \n",
    "        # Ajuda no calculo do erro médio quadrado\n",
    "        erros_interacao = []\n",
    "        \n",
    "        for j in range(len(x)): # para cada exemplo\n",
    "            \n",
    "            # Passo 3 -  calcule a saída do neurônio (~1 linha)\n",
    "            y_calc = predict(forward(w,b,x[j]))\n",
    "            \n",
    "            # Passo 4 - calcule o erro entre a saída obtida e a saída desejada nos rótulos/labels (~1 linha)\n",
    "            erro = y[j] - y_calc  \n",
    "            \n",
    "            # Adiciona o erro quadrado dessa instancia\n",
    "            erros_interacao.append(erro*erro)\n",
    "            \n",
    "            # Ajusta os pesos e bias\n",
    "            for idx in range(len(w)):\n",
    "                w[idx] = w[idx] + erro*learning_rate*x[j][idx]            \n",
    "\n",
    "            b = b + erro*learning_rate\n",
    "        \n",
    "        # Calcula o erro médio dessa interação\n",
    "        erro_medio = np.average(erros_interacao)\n",
    "        \n",
    "        training_interation.append(i)\n",
    "        training_erro.append(erro_medio) \n",
    "        \n",
    "        # Apenas para garantir que pare quando o erro atingir um valor limite mínimo\n",
    "        if (erro_medio < 0.00001):\n",
    "            print(\"Finalizado na interação %d visto que o erro médio já se tornou muito pequeno %f\" % (i, erro_medio))\n",
    "            break;\n",
    "    \n",
    "    # Cria um grafico com os dados sobre o erro de treinamento\n",
    "    pylab.rcParams['figure.figsize'] = (15.0, 10.0)\n",
    "    plt.plot(training_interation, training_erro)\n",
    "    plt.xlabel('Interação')\n",
    "    plt.ylabel('Erro médio quadrado')\n",
    "    plt.show()\n",
    "    \n",
    "    return w,b\n",
    "\n",
    "def validar(w,b,x,y):\n",
    "\n",
    "    # Preenche o array de saidas preditas\n",
    "    y_pred = []\n",
    "    for j in range(len(x)):\n",
    "        y_pred.append(predict(forward(w,b,x[j])))\n",
    "       \n",
    "    # Métricas de Avaliação\n",
    "    print('Matriz de Confusão:')\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    print('F1 Score:')\n",
    "    print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação dos Datasets de Treinamento e Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = criaDataset(800, 0.1)\n",
    "X_val, Y_val = criaDataset(200, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainamento do Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = perceptron(X_train, Y_train, 1000, 0.001)\n",
    "print(\"Pesos Aprendidos:\", w, \"\\nBias Aprendido:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjunto de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validar(w, b, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conjunto de Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validar(w, b, X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
